# SPDX-License-Identifier: GPL-2.0-only
config NVME_CORE
	tristate

config BLK_DEV_NVME
	tristate "NVM Express block device"
	depends on PCI && BLOCK
	select NVME_CORE
	---help---
	  The NVM Express driver is for solid state drives directly
	  connected to the PCI or PCI Express bus.  If you know you
	  don't have one of these, it is safe to answer N.

	  To compile this driver as a module, choose M here: the
	  module will be called nvme.

config NVME_MULTIPATH
	bool "NVMe multipath support"
	depends on NVME_CORE
	---help---
	   This option enables support for multipath access to NVMe
	   subsystems.  If this option is enabled only a single
	   /dev/nvmeXnY device will show up for each NVMe namespaces,
	   even if it is accessible through multiple controllers.

config NVME_FABRICS
	tristate

config NVME_RDMA
	tristate "NVM Express over Fabrics RDMA host driver"
	depends on INFINIBAND && INFINIBAND_ADDR_TRANS && BLOCK
	select NVME_CORE
	select NVME_FABRICS
	select SG_POOL
	help
	  This provides support for the NVMe over Fabrics protocol using
	  the RDMA (Infiniband, RoCE, iWarp) transport.  This allows you
	  to use remote block devices exported using the NVMe protocol set.

	  To configure a NVMe over Fabrics controller use the nvme-cli tool
	  from https://github.com/linux-nvme/nvme-cli.

	  If unsure, say N.

config NVME_FC
	tristate "NVM Express over Fabrics FC host driver"
	depends on BLOCK
	depends on HAS_DMA
	select NVME_CORE
	select NVME_FABRICS
	select SG_POOL
	help
	  This provides support for the NVMe over Fabrics protocol using
	  the FC transport.  This allows you to use remote block devices
	  exported using the NVMe protocol set.

	  To configure a NVMe over Fabrics controller use the nvme-cli tool
	  from https://github.com/linux-nvme/nvme-cli.

	  If unsure, say N.

config NVME_TCP
	tristate "NVM Express over Fabrics TCP host driver"
	depends on INET
	depends on BLK_DEV_NVME
	select NVME_FABRICS
	help
	  This provides support for the NVMe over Fabrics protocol using
	  the TCP transport.  This allows you to use remote block devices
	  exported using the NVMe protocol set.

	  To configure a NVMe over Fabrics controller use the nvme-cli tool
	  from https://github.com/linux-nvme/nvme-cli.

	  If unsure, say N.

config IOSCHED_D2FQ
	bool "Device-Direct Fair Queueing (D2FQ)"
	help
	  This provides fair I/O scheduling for NVMe device using 
	  WRRU arbitration.

config IOSCHED_D2FQ_MULTISQ
	bool "Three classes of SQ per each io queue pair"
	depends on IOSCHED_D2FQ
	default n
	help
	  This allows each core to have isolated three SQs (low, med, high). 
	  This option works only when the number of NVMe queues is three times
	  more than the number of cores. Without this config, three adjacent cores
	  share the three SQs (low, med, high).

config IOSCHED_D2FQ_VERBOSE
	depends on IOSCHED_D2FQ
	default n
	bool "Verbose configuration for D2FQ"
	help
	  This provides debuging configuration for NVMe device fair scheduling 
	  using WRRU arbitration.


